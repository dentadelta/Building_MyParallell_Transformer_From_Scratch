{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c227813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from Config import conf\n",
    "from transformers import T5Tokenizer\n",
    "from torchtext.nn.modules.multiheadattention import ScaledDotProduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76ba0a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = conf()\n",
    "h = config.h\n",
    "N = config.N\n",
    "dmodel = config.dmodel\n",
    "dk= config.dk\n",
    "dv = config.dv\n",
    "dff = config.dff\n",
    "tokenizer = T5Tokenizer.from_pretrained(config.tokenizer_path)\n",
    "max_length = config.max_length\n",
    "vocab_size = config.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fb858ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1input = 'I love dog'\n",
    "sentence2input = 'I love cat'\n",
    "sentence1output = 'dog'\n",
    "sentence2output = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c565b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input = tokenizer.batch_encode_plus([sentence1input,sentence2input],\n",
    "                                          max_length= max_length,\n",
    "                                          truncation=True,\n",
    "                                          return_tensors='pt'\n",
    "                                         )\n",
    "batch_output = tokenizer.batch_encode_plus([sentence1output,sentence2output],max_length= max_length, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "341d3247",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(vocab_size, d_model)\n",
    "        position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.embedded_layer = nn.Embedding(vocab_size,dmodel)\n",
    "    def forward(self, x):\n",
    "        x = self.embedded_layer(x)\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77be7b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttentionHead(nn.Module):\n",
    "    def __init__(self,dmodel,dk,dv,cuda_number='cuda:0'):\n",
    "        super(SingleAttentionHead,self).__init__()\n",
    "        self.proj_key = nn.Linear(dmodel,dk).to(cuda_number)\n",
    "        self.proj_query = nn.Linear(dmodel,dk).to(cuda_number)\n",
    "        self.proj_value  = nn.Linear(dmodel,dv).to(cuda_number)\n",
    "        self.dk = dk\n",
    "        self.cuda_number = cuda_number\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.to(self.cuda_number)\n",
    "        k = self.proj_key(x)\n",
    "        q = self.proj_query(x)\n",
    "        v = self.proj_value(x)\n",
    "        head = torch.matmul(F.softmax(torch.matmul(q,k.transpose(-2,-1))/(self.dk**0.5)),v)\n",
    "        if self.cuda_number != 'cuda:0':\n",
    "            return head.to('cuda:0')\n",
    "        return head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfcf747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttentionHead(nn.Module):\n",
    "    def __init__(self,dmodel,dk,dv,maximum_sequence_length):\n",
    "        super(MultiAttentionHead, self).__init__()\n",
    "        \n",
    "        nlayers_GPU_0 = 4\n",
    "        nlayers_GPU_1 = 4\n",
    "        \n",
    "        self.head_GPU0 = nn.ModuleList([\n",
    "            SingleAttentionHead(dmodel,dk,dv,'cuda:0') for i in range(nlayers_GPU_0)\n",
    "        ])\n",
    "        \n",
    "        self.head_GPU1 = nn.ModuleList([\n",
    "            SingleAttentionHead(dmodel,dk,dv,'cuda:1') for i in range(nlayers_GPU_1)\n",
    "        ])\n",
    "        #Weight_0 layer:\n",
    "        self.W0 = nn.Linear(dmodel,dmodel).to('cuda:0')   #Size h*dv x dmodel. But since dv = dk and dk x h = dv so it's a dmodel x dmodel layer -> cuda:0\n",
    "        #LayerNormalisation\n",
    "        self.Add_and_Nom = nn.LayerNorm(dmodel, eps=1e-05, elementwise_affine=True).to('cuda:0')\n",
    "        self.dropout = nn.Dropout(0.1).to('cuda:0')\n",
    "    \n",
    "    def forward(self,x):\n",
    "        multi_attention_heads = 'Empty'\n",
    "        for i, l in enumerate(self.head_GPU0):\n",
    "            if i == 0:\n",
    "                multi_attention_heads = l(x)\n",
    "            else:\n",
    "                multi_attention_heads = torch.cat((multi_attention_heads,l(x)), dim=2)\n",
    "        for i, l in enumerate(self.head_GPU1):\n",
    "            multi_attention_heads = torch.cat((multi_attention_heads,l(x)), dim=2)\n",
    "        multi_attention_heads = self.W0(multi_attention_heads) \n",
    "        multi_attention_heads = self.Add_and_Nom(x + multi_attention_heads)  #cuda:0\n",
    "        multi_attention_heads = self.dropout(multi_attention_heads)\n",
    "        return multi_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37243dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStack(nn.Module):\n",
    "    def __init__(self,dmodel,dk,dv,maximum_sequence_length,vocab_size):\n",
    "        super(EncoderStack, self).__init__()\n",
    "        \n",
    "        self.multiAttentionHeads = MultiAttentionHead(dmodel,dk,dv,max_length)\n",
    "        self.lin1a = nn.Linear(dmodel,dff).to('cuda:0')\n",
    "        self.dropout1 = nn.Dropout(0.1).to('cuda:0')\n",
    "        self.lin1b = nn.Linear(dff,dmodel).to('cuda:0')\n",
    "        self.Add_and_Nom = nn.LayerNorm(dmodel, eps=1e-05, elementwise_affine=True).to('cuda:0')\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.multiAttentionHeads(x)\n",
    "        sublayer_x = self.lin1a(x)\n",
    "        sublayer_x = F.relu(sublayer_x)\n",
    "        sublayer_x = self.dropout1(sublayer_x)\n",
    "        sublayer_x = self.lin1b(sublayer_x)\n",
    "        sublayer_x = self.Add_and_Nom(x + sublayer_x)\n",
    "        return sublayer_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fb93747",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderTransformerStacks(nn.Module):\n",
    "    def __init__(self,dmodel,dk,dv,maximum_sequence_length,vocab_size):\n",
    "        super(EncoderTransformerStacks, self).__init__()\n",
    "        self.encoderStack = nn.ModuleList([\n",
    "            EncoderStack(dmodel,dk,dv,max_length,vocab_size) for i in range(6)\n",
    "        ])\n",
    "\n",
    "    def forward(self,x):\n",
    "        for i, l in enumerate(self.encoderStack):\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "106ba5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderTransformer(nn.Module):\n",
    "    def __init__(self,dmodel,dk,dv,maximum_sequence_length,vocab_size):\n",
    "        super(EncoderTransformer, self).__init__()\n",
    "        self.positionEncoder = PositionalEncoding(dmodel,0.1, vocab_size).to('cuda:0')\n",
    "        self.encoder_Stacks = EncoderTransformerStacks(dmodel,dk,dv,max_length,vocab_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.positionEncoder(x)\n",
    "        x = self.encoder_Stacks(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70b2032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = batch_input['input_ids'].to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a336b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Encode = EncoderTransformer(dmodel,dk,dv,max_length,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5cc8786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.4100,  0.0163,  0.3636,  ...,  0.0301, -1.2281, -0.2496],\n",
      "         [-0.6611,  0.2060, -0.4789,  ..., -0.9819, -0.2055, -0.1703],\n",
      "         [-1.0295, -0.5003, -0.1525,  ...,  0.6403, -0.9625, -1.3378],\n",
      "         [-0.4530, -0.3288, -1.0474,  ...,  1.5810, -1.5054, -0.8711]],\n",
      "\n",
      "        [[-1.9617, -0.1181, -0.4805,  ..., -0.8597, -0.4226, -0.8068],\n",
      "         [-0.8208,  0.7501, -0.0249,  ...,  0.0136, -0.8985, -0.7037],\n",
      "         [-0.8347,  1.5114, -0.8424,  ...,  0.9458, -0.7744, -0.1376],\n",
      "         [-0.0605,  0.7210,  0.2302,  ...,  0.0560, -0.7479, -0.2370]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "out = Encode(input_ids)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aef23ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_0_Memory_Allocated = torch.cuda.memory_reserved(0)\n",
    "GPU_1_Memory_Allocated = torch.cuda.memory_reserved(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21e2f85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used in GPU:0 1.66 GB\n",
      "Memory used in GPU:1 1.05 GB\n"
     ]
    }
   ],
   "source": [
    "print('Memory used in GPU:0', round((GPU_0_Memory_Allocated)/100000000,2),'GB')\n",
    "print('Memory used in GPU:1', round((GPU_1_Memory_Allocated)/10000000,2),'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c727898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU:0 was using 400MB before so both GPUs are actually sharing the same model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c1428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
