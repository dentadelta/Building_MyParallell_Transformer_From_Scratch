{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c227813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from Config import conf\n",
    "from transformers import T5Tokenizer\n",
    "from torchtext.nn.modules.multiheadattention import ScaledDotProduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76ba0a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = conf()\n",
    "h = config.h\n",
    "N = config.N\n",
    "dmodel = config.dmodel\n",
    "dk= config.dk\n",
    "dv = config.dv\n",
    "dff = config.dff\n",
    "tokenizer = T5Tokenizer.from_pretrained(config.tokenizer_path)\n",
    "max_length = config.max_length\n",
    "vocab_size = config.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fb858ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1input = 'I love dog'\n",
    "sentence2input = 'I love cat'\n",
    "sentence3input = 'I love money'\n",
    "sentence4input = 'I love overtime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c565b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_batch_input = tokenizer.batch_encode_plus([sentence1input,sentence2input,sentence3input,sentence4input],\n",
    "                                          max_length= max_length,\n",
    "                                          truncation=True,\n",
    "                                          return_tensors='pt'\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "341d3247",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len, vocab_size):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embedded_layer = nn.Embedding(vocab_size,d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedded_layer(x)\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31a6ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttentionHead(nn.Module):\n",
    "    def __init__(self,dmodel,dk,dv,cuda_number='cuda:0'):\n",
    "        super(SingleAttentionHead,self).__init__()\n",
    "        self.proj_key = nn.Linear(dmodel,dk).to(cuda_number)\n",
    "        self.proj_query = nn.Linear(dmodel,dk).to(cuda_number)\n",
    "        self.proj_value  = nn.Linear(dmodel,dv).to(cuda_number)\n",
    "        self.dk = dk\n",
    "        self.cuda_number = cuda_number\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.to(self.cuda_number)\n",
    "        k = self.proj_key(x)\n",
    "        q = self.proj_query(x)\n",
    "        v = self.proj_value(x)\n",
    "        head = torch.matmul(F.softmax(torch.matmul(q,k.transpose(-2,-1))/(self.dk**0.5)),v)\n",
    "        if self.cuda_number != 'cuda:0':\n",
    "            return head.to('cuda:0')\n",
    "        return head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfcf747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttentionHead(nn.Module):\n",
    "    def __init__(self,dmodel,dk,dv):\n",
    "        super(MultiAttentionHead, self).__init__()\n",
    "        \n",
    "        nlayers_GPU_0 = int(h/2)\n",
    "        nlayers_GPU_1 = int(h/2)\n",
    "        \n",
    "        self.head_GPU0 = nn.ModuleList([\n",
    "            SingleAttentionHead(dmodel,dk,dv,'cuda:0') for i in range(nlayers_GPU_0)\n",
    "        ])\n",
    "        \n",
    "        self.head_GPU1 = nn.ModuleList([\n",
    "            SingleAttentionHead(dmodel,dk,dv,'cuda:1') for i in range(nlayers_GPU_1)\n",
    "        ])\n",
    "        #Weight_0 layer:\n",
    "        self.W0 = nn.Linear(dmodel,dmodel).to('cuda:0')   #Size h*dv x dmodel. But since dv = dk and dk x h = dv so it's a dmodel x dmodel layer -> cuda:0\n",
    "        #LayerNormalisation\n",
    "        self.Add_and_Nom = nn.LayerNorm(dmodel, eps=1e-05, elementwise_affine=True).to('cuda:0')\n",
    "        self.dropout = nn.Dropout(0.1).to('cuda:0')\n",
    "    \n",
    "    def forward(self,x):\n",
    "        multi_attention_heads = 'Empty'\n",
    "        for i, l in enumerate(self.head_GPU0):\n",
    "            if i == 0:\n",
    "                multi_attention_heads = l(x)\n",
    "            else:\n",
    "                multi_attention_heads = torch.cat((multi_attention_heads,l(x)), dim=2)\n",
    "        for i, l in enumerate(self.head_GPU1):\n",
    "            multi_attention_heads = torch.cat((multi_attention_heads,l(x)), dim=2)\n",
    "        multi_attention_heads = self.W0(multi_attention_heads) \n",
    "        multi_attention_heads = self.Add_and_Nom(x + multi_attention_heads)  #cuda:0\n",
    "        multi_attention_heads = self.dropout(multi_attention_heads)\n",
    "        return multi_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "869c2fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStack(nn.Module):\n",
    "    def __init__(self,dmodel,dk,dv):\n",
    "        super(EncoderStack, self).__init__()\n",
    "        self.multiAttentionHeads = MultiAttentionHead(dmodel,dk,dv)\n",
    "        self.lin1a = nn.Linear(dmodel,dff).to('cuda:0')\n",
    "        self.dropout1 = nn.Dropout(0.1).to('cuda:0')\n",
    "        self.lin1b = nn.Linear(dff,dmodel).to('cuda:0')\n",
    "        self.Add_and_Nom = nn.LayerNorm(dmodel, eps=1e-05, elementwise_affine=True).to('cuda:0')\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.multiAttentionHeads(x)\n",
    "        sublayer_x = self.lin1a(x)\n",
    "        sublayer_x = F.relu(sublayer_x)\n",
    "        sublayer_x = self.dropout1(sublayer_x)\n",
    "        sublayer_x = self.lin1b(sublayer_x)\n",
    "        sublayer_x = self.Add_and_Nom(x + sublayer_x)\n",
    "        return sublayer_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b316932",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderTransformerStacks(nn.Module):\n",
    "    def __init__(self,dmodel,dk,dv):\n",
    "        super(EncoderTransformerStacks, self).__init__()\n",
    "        self.encoderStack = nn.ModuleList([\n",
    "            EncoderStack(dmodel,dk,dv) for i in range(6)\n",
    "        ])\n",
    "\n",
    "    def forward(self,x):\n",
    "        for i, l in enumerate(self.encoderStack):\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "068767d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderTransformer(nn.Module):\n",
    "    def __init__(self,dmodel,dk,dv,max_length,vocab_size):\n",
    "        super(EncoderTransformer, self).__init__()\n",
    "        self.positionEncoder = PositionalEncoding(dmodel,0.1, max_length,vocab_size).to('cuda:0')\n",
    "        self.encoder_Stacks = EncoderTransformerStacks(dmodel,dk,dv)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.positionEncoder(x)\n",
    "        x = self.encoder_Stacks(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca68dec6",
   "metadata": {},
   "source": [
    "### Decoder Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c97f178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(input_ids):\n",
    "    ms =output1d_.size(1)\n",
    "    input_ids = torch.cat(ms*[input_ids],dim=0).view(ms,-1)\n",
    "    return torch.flip(torch.triu(input_ids), [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d856daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSingleAttentionHead(nn.Module):\n",
    "    def __init__(self,dmodel,dk,dv,cuda_number='cuda:0'):\n",
    "        super(MaskedSingleAttentionHead,self).__init__()\n",
    "        self.proj_key = nn.Linear(dmodel,dk).to(cuda_number)\n",
    "        self.proj_query = nn.Linear(dmodel,dk).to(cuda_number)\n",
    "        self.proj_value  = nn.Linear(dmodel,dv).to(cuda_number)\n",
    "        self.dk = dk\n",
    "        self.cuda_number = cuda_number\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.to(self.cuda_number)\n",
    "        k = self.proj_key(x)\n",
    "        q = self.proj_query(x)\n",
    "        v = self.proj_value(x)\n",
    "        q_size = q.size(1)\n",
    "        mask = (torch.triu(torch.ones(q_size, q_size)) == 1).transpose(-2, -1).to(self.cuda_number)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        k_QT = torch.matmul(k,q.transpose(-2,-1))\n",
    "        masked = k_QT + mask\n",
    "        head = torch.matmul(F.softmax(masked/(dk**0.5)),v)\n",
    "        if self.cuda_number != 'cuda:0':\n",
    "            return head.to('cuda:0')\n",
    "        return head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c3ae3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_batch_input = tokenizer.batch_encode_plus(['I love a man','I love a woman','I am not a bi','I am only joking'],\n",
    "                                           max_length= max_length,\n",
    "                                           padding=True,\n",
    "                                           truncation=True,\n",
    "                                           return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "121aa991",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = decoder_batch_input['input_ids']\n",
    "inputs = inputs.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9923940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_attention_head_for_each_sequence(input_):\n",
    "    pos = PositionalEncoding(dmodel, 0.1, max_length, vocab_size).to('cuda:0')\n",
    "    masked_input = pos(inputs)\n",
    "    attentionhead1 = MaskedSingleAttentionHead(dmodel,dk,dv,cuda_number='cuda:0')\n",
    "    attentionhead2 = MaskedSingleAttentionHead(dmodel,dk,dv,cuda_number='cuda:0')\n",
    "    attentionhead3 = MaskedSingleAttentionHead(dmodel,dk,dv,cuda_number='cuda:0')\n",
    "    attentionhead4 = MaskedSingleAttentionHead(dmodel,dk,dv,cuda_number='cuda:0')\n",
    "    attentionhead5 = MaskedSingleAttentionHead(dmodel,dk,dv,cuda_number='cuda:1')\n",
    "    attentionhead6 = MaskedSingleAttentionHead(dmodel,dk,dv,cuda_number='cuda:1')\n",
    "    attentionhead7 = MaskedSingleAttentionHead(dmodel,dk,dv,cuda_number='cuda:1')\n",
    "    attentionhead8 = MaskedSingleAttentionHead(dmodel,dk,dv,cuda_number='cuda:1')\n",
    "    head1 = attentionhead1(masked_input)\n",
    "    head2 = attentionhead2(masked_input)\n",
    "    head3 = attentionhead3(masked_input)\n",
    "    head4 = attentionhead4(masked_input)\n",
    "    head5 = attentionhead5(masked_input)\n",
    "    head6 = attentionhead6(masked_input)\n",
    "    head7 = attentionhead7(masked_input)\n",
    "    head8 = attentionhead8(masked_input)\n",
    "    head1 = head1[:,max_length - 1,]\n",
    "    head2 = head2[:,max_length - 1,]\n",
    "    head3 = head3[:,max_length - 1,]\n",
    "    head4 = head4[:,max_length - 1,]\n",
    "    head5 = head5[:,max_length - 1,]\n",
    "    head6 = head6[:,max_length - 1,]\n",
    "    head7 = head7[:,max_length - 1,]\n",
    "    head8 = head8[:,max_length - 1,]\n",
    "    heads = torch.cat((head1,head2,head3,head4,head5,head6,head7,head8),dim=-1)\n",
    "    return heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4fa2ed",
   "metadata": {},
   "source": [
    "#### Encoder outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4ab33be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs = encoder_batch_input['input_ids'].to('cuda:0')\n",
    "Encoder = EncoderTransformer(dmodel,dk,dv,max_length,vocab_size)\n",
    "Encoder_output = Encoder(encoder_inputs)\n",
    "Encoder_output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf1caea",
   "metadata": {},
   "source": [
    "#### Decoder masked multihead  outputs -->Not sure it's correct yet!!! -> One error is all you need!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfc68d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There are 4 sequences in this batch\n",
    "sequence_1 = generate_attention_head_for_each_sequence(inputs[0])\n",
    "sequence_2 = generate_attention_head_for_each_sequence(inputs[1])\n",
    "sequence_3 = generate_attention_head_for_each_sequence(inputs[2])\n",
    "sequence_4 = generate_attention_head_for_each_sequence(inputs[3])\n",
    "\n",
    "masked_multihead_attention_batch_out = torch.cat((sequence_1,sequence_2,sequence_3,sequence_4)).view(inputs.size(0),-1,dmodel)\n",
    "masked_multihead_attention_batch_out.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
