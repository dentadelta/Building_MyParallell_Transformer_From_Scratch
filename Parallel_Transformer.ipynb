{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c227813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from Config import conf\n",
    "from transformers import T5Tokenizer\n",
    "from torchtext.nn.modules.multiheadattention import ScaledDotProduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76ba0a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = conf()\n",
    "h = config.h\n",
    "N = config.N\n",
    "dmodel = config.dmodel\n",
    "dk= config.dk\n",
    "dv = config.dv\n",
    "dff = config.dff\n",
    "tokenizer = T5Tokenizer.from_pretrained(config.tokenizer_path)\n",
    "max_length = config.max_length\n",
    "vocab_size = config.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fb858ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1input = 'I love dog'\n",
    "sentence2input = 'I love cat'\n",
    "sentence3input = 'I love money'\n",
    "sentence4input = 'I love overtime'\n",
    "\n",
    "decoder1input_ = 'dog meat is delicious'\n",
    "sentence2input_ = 'cat meat is bad '\n",
    "sentence3input_ = 'I can buy dogs'\n",
    "sentence4input_ = 'I can buy cats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c565b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = tokenizer.batch_encode_plus([sentence1input,sentence2input,sentence3input,sentence4input],\n",
    "                                          max_length= max_length,\n",
    "                                          pad_to_max_length = True,\n",
    "                                          truncation=True,\n",
    "                                          return_tensors='pt'\n",
    "                                         )\n",
    "encoder_inputs = encoder_inputs['input_ids'].to('cuda:0')\n",
    "\n",
    "\n",
    "decoder_inputs = tokenizer.batch_encode_plus([sentence1input,sentence2input,sentence3input,sentence4input],\n",
    "                                          max_length= max_length,\n",
    "                                          pad_to_max_length = True,\n",
    "                                          truncation=True,\n",
    "                                          return_tensors='pt'\n",
    "                                         )\n",
    "decoder_inputs = decoder_inputs['input_ids'].to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52f9498f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 1, 1]], device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "def create_mask(sequence_length,cuda_number):\n",
    "    mask = (torch.triu(torch.ones(sequence_length, sequence_length)) == 1).transpose(-2, -1).to(cuda_number)\n",
    "    mask = mask.int().masked_fill(mask == 0, 0)\n",
    "    return mask\n",
    "\n",
    "#example:\n",
    "example_mask = create_mask(4,'cuda:0')\n",
    "print(example_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "341d3247",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len, vocab_size):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embedded_layer = nn.Embedding(vocab_size,d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedded_layer(x)\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31a6ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttentionHead(nn.Module):\n",
    "    def __init__(self,dmodel,dk,dv,max_length,cuda_number='cuda:0', applyMask = False):\n",
    "        super(SingleAttentionHead,self).__init__()\n",
    "        self.proj_key = nn.Linear(dmodel,dk).to(cuda_number)\n",
    "        self.proj_query = nn.Linear(dmodel,dk).to(cuda_number)\n",
    "        self.proj_value  = nn.Linear(dmodel,dv).to(cuda_number)\n",
    "        self.dk = dk\n",
    "        self.cuda_number = cuda_number\n",
    "        self.max_length = max_length\n",
    "        self.applyMask = applyMask\n",
    "        \n",
    "    def forward(self,x,y=None):\n",
    "        x = x.to(self.cuda_number)\n",
    "        k = self.proj_key(x)\n",
    "        if y == None: #If you dont supply a y value value then this is the self attended layer\n",
    "            q = self.proj_query(x)\n",
    "            v = self.proj_value(x)\n",
    "            \n",
    "        if y != None:  # If you need a mask then this is the encoder-decoder attention layer\n",
    "            y = y.to(self.cuda_number)\n",
    "            q = self.proj_query(y)  #y is encoder output, you get the query from the encoder\n",
    "            v = self.proj_value(y)  #y is the encoder output, you get the key from the encoder\n",
    "        \n",
    "        I = torch.einsum('b i d , b j d -> b i j', q, k)\n",
    "        \n",
    "        if self.applyMask and y == None: #If you need a mask then this is the decoder-self attended layer\n",
    "            mask = create_mask(self.max_length,self.cuda_number)\n",
    "            for i in range(len(I)):\n",
    "                I[i].masked_fill_(mask==0,float('-inf'))\n",
    "        \n",
    "        attention = F.softmax(I/(self.dk**0.5), dim=-1)\n",
    "        \n",
    "        head = torch.einsum('b i j , b j d -> b i d', attention, v)\n",
    "        \n",
    "        if self.cuda_number != 'cuda:0':\n",
    "            return head.to('cuda:0')\n",
    "        return head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9429b99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttentionHead(nn.Module):\n",
    "    def __init__(self,dmodel,dk,dv,max_length,applyMask = False):\n",
    "        super(MultiAttentionHead, self).__init__()\n",
    "        \n",
    "        nlayers_GPU_0 = int(h/2)\n",
    "        nlayers_GPU_1 = int(h/2)\n",
    "        \n",
    "        self.head_GPU0 = nn.ModuleList([\n",
    "            SingleAttentionHead(dmodel,dk,dv,max_length,'cuda:0',applyMask) for i in range(nlayers_GPU_0)\n",
    "        ])\n",
    "        \n",
    "        self.head_GPU1 = nn.ModuleList([\n",
    "            SingleAttentionHead(dmodel,dk,dv,max_length,'cuda:1',applyMask) for i in range(nlayers_GPU_1)\n",
    "        ])\n",
    "        #Weight_0 layer:\n",
    "        self.W0 = nn.Linear(dmodel,dmodel).to('cuda:0')   #Size h*dv x dmodel. But since dv = dk and dk x h = dv so it's a dmodel x dmodel layer -> cuda:0\n",
    "        #LayerNormalisation\n",
    "        self.Add_and_Nom = nn.LayerNorm(dmodel, eps=1e-05, elementwise_affine=True).to('cuda:0')\n",
    "        self.dropout = nn.Dropout(0.1).to('cuda:0')\n",
    "    \n",
    "    def forward(self,x,y=None):\n",
    "        multi_attention_heads = 'Empty'\n",
    "        for i, l in enumerate(self.head_GPU0):\n",
    "            if i == 0:\n",
    "                multi_attention_heads = l(x,y)\n",
    "            else:\n",
    "                multi_attention_heads = torch.cat((multi_attention_heads,l(x,y)), dim=2)\n",
    "        for i, l in enumerate(self.head_GPU1):\n",
    "            multi_attention_heads = torch.cat((multi_attention_heads,l(x,y)), dim=2)\n",
    "        multi_attention_heads = self.W0(multi_attention_heads) \n",
    "        multi_attention_heads = self.Add_and_Nom(x + multi_attention_heads)  #cuda:0\n",
    "        multi_attention_heads = self.dropout(multi_attention_heads)\n",
    "        return multi_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf50449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStack(nn.Module):\n",
    "    def __init__(self,dmodel,dk,dv,max_length):\n",
    "        super(EncoderStack, self).__init__()\n",
    "        self.multiAttentionHeads = MultiAttentionHead(dmodel,dk,dv,max_length,False)\n",
    "        self.lin1a = nn.Linear(dmodel,dff).to('cuda:0')\n",
    "        self.dropout1 = nn.Dropout(0.1).to('cuda:0')\n",
    "        self.lin1b = nn.Linear(dff,dmodel).to('cuda:0')\n",
    "        self.Add_and_Nom = nn.LayerNorm(dmodel, eps=1e-05, elementwise_affine=True).to('cuda:0')\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.multiAttentionHeads(x)\n",
    "        sublayer_x = self.lin1a(x)\n",
    "        sublayer_x = F.relu(sublayer_x)\n",
    "        sublayer_x = self.dropout1(sublayer_x)\n",
    "        sublayer_x = self.lin1b(sublayer_x)\n",
    "        sublayer_x = self.Add_and_Nom(x + sublayer_x)\n",
    "        return sublayer_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "850754ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self,dmodel,dk,dv,max_length):\n",
    "        super(DecoderStack, self).__init__()\n",
    "        self.masked_multi_head_attention = MultiAttentionHead(dmodel,dk,dv,max_length,True)\n",
    "        self.multi_head_attention = MultiAttentionHead(dmodel,dk,dv,max_length,False)\n",
    "        self.lin1a = nn.Linear(dmodel,dff).to('cuda:0')\n",
    "        self.dropout1 = nn.Dropout(0.1).to('cuda:0')\n",
    "        self.lin1b = nn.Linear(dff,dmodel).to('cuda:0')\n",
    "        self.Add_and_Nom = nn.LayerNorm(dmodel, eps=1e-05, elementwise_affine=True).to('cuda:0')\n",
    "\n",
    "    def forward(self,x,y=None):\n",
    "        z = self.masked_multi_head_attention(x)\n",
    "        z = self.multi_head_attention(x,y)\n",
    "        sublayer_z = self.lin1a(z)\n",
    "        sublayer_z = F.relu(sublayer_z)\n",
    "        sublayer_z = self.dropout1(sublayer_z)\n",
    "        sublayer_z = self.lin1b(sublayer_z)\n",
    "        sublayer_z = self.Add_and_Nom(z + sublayer_z)\n",
    "        return sublayer_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbcadfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderTransformerStacks(nn.Module):\n",
    "    def __init__(self,dmodel,dk,dv,max_length):\n",
    "        super(EncoderTransformerStacks, self).__init__()\n",
    "        self.encoderStack = nn.ModuleList([\n",
    "            EncoderStack(dmodel,dk,dv,max_length) for i in range(6)\n",
    "        ])\n",
    "\n",
    "    def forward(self,x):\n",
    "        for i, l in enumerate(self.encoderStack):\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26889eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderTransformerStacks(nn.Module):\n",
    "    def __init__(self,dmodel,dk,dv,max_length):\n",
    "        super(DecoderTransformerStacks, self).__init__()\n",
    "        self.dencoderStack = nn.ModuleList([\n",
    "            DecoderStack(dmodel,dk,dv,max_length) for i in range(6)\n",
    "        ])\n",
    "\n",
    "    def forward(self,d,e):\n",
    "        for i, l in enumerate(self.dencoderStack):\n",
    "            x = l(d,e)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0832baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderTransformer(nn.Module):\n",
    "    def __init__(self,dmodel,dk,dv,max_length,vocab_size):\n",
    "        super(EncoderTransformer, self).__init__()\n",
    "        self.positionEncoder = PositionalEncoding(dmodel,0.1, max_length,vocab_size).to('cuda:0')\n",
    "        self.encoder_Stacks = EncoderTransformerStacks(dmodel,dk,dv,max_length)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.positionEncoder(x)\n",
    "        x = self.encoder_Stacks(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5da4a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(self,dmodel,dk,dv,max_length,vocab_size):\n",
    "        super(DecoderTransformer, self).__init__()\n",
    "        self.positionEncoder = PositionalEncoding(dmodel,0.1, max_length,vocab_size).to('cuda:0')\n",
    "        self.decoder_Stacks = DecoderTransformerStacks(dmodel,dk,dv,max_length)\n",
    "        \n",
    "    def forward(self,d,e):\n",
    "        d = self.positionEncoder(d)\n",
    "        x = self.decoder_Stacks(d,e)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8aa6421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decoder_batch_sequence(decoder_inputs):\n",
    "    mask = create_mask(decoder_inputs.size(1),'cuda:0')\n",
    "    new_sequence = 'empty'\n",
    "    for i in range(len(decoder_inputs)):\n",
    "        decoder_sequence = torch.cat(max_length*[decoder_inputs[i]]).reshape(max_length,-1)\n",
    "        decoder_sequence = decoder_sequence.masked_fill_(mask==0,0)\n",
    "        if i == 0:\n",
    "            new_sequence = decoder_sequence\n",
    "        else:\n",
    "            new_sequence = torch.cat((new_sequence,decoder_sequence),dim=0)\n",
    "    return new_sequence.view(decoder_inputs.size(0),decoder_inputs.size(1),-1).permute(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8db6229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_correct_output(decoder_inputs):\n",
    "    correct_outputs = 'empty'\n",
    "    for i in range(decoder_inputs.size(1)):\n",
    "        if i < decoder_inputs.size(1) - 1:\n",
    "            output = decoder_inputs[:,i+1]\n",
    "            if i == 0:\n",
    "                correct_outputs = output\n",
    "            else:\n",
    "                correct_outputs = torch.cat((correct_outputs,output),dim=0)\n",
    "    return correct_outputs.reshape(-1,decoder_inputs.size(0)).transpose(-2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6bc07d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_decoder_input_sequence_per_encoder_batch(decoder_inputs):\n",
    "    input_sequence = create_decoder_batch_sequence(decoder_inputs)\n",
    "    next_tokens = create_correct_output(decoder_inputs)\n",
    "    return {'input_ids': input_sequence,\n",
    "            'output_ids':next_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f9a7d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformer(nn.Module):\n",
    "    def __init__(self,dmodel,dk,dv,max_length,vocab_size):\n",
    "        super(CustomTransformer,self).__init__()\n",
    "        self.encoder = EncoderTransformer(dmodel,dk,dv,max_length,vocab_size)\n",
    "        self.decoder = DecoderTransformer(dmodel,dk,dv,max_length,vocab_size)\n",
    "        self.linear = nn.Linear(dmodel,vocab_size).to('cuda:0')\n",
    "    \n",
    "    def forward(self,e,d):\n",
    "        e = self.encoder(e)\n",
    "        d = self.decoder(d,e)\n",
    "        d = self.linear(d)\n",
    "        d = d.mean(dim=1)\n",
    "        return F.log_softmax(d,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "129a2c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "customTransformer = CustomTransformer(dmodel,dk,dv,max_length,vocab_size)\n",
    "Criterion = nn.CrossEntropyLoss()\n",
    "Optimiser = torch.optim.Adam(customTransformer.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e764681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingBatch = [[encoder_inputs,decoder_inputs]]   # For simplicity, lets try out with only one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e47f363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  21.8384\n",
      "Epoch:  10 Loss:  2.5195\n",
      "Epoch:  20 Loss:  1.4578\n",
      "Epoch:  30 Loss:  1.3344\n",
      "Epoch:  40 Loss:  0.7253\n",
      "Epoch:  50 Loss:  0.5453\n",
      "Epoch:  60 Loss:  0.4437\n",
      "Epoch:  70 Loss:  0.2887\n",
      "Epoch:  80 Loss:  0.2391\n",
      "Epoch:  90 Loss:  0.1771\n"
     ]
    }
   ],
   "source": [
    "customTransformer.train()\n",
    "for epoch in range(100):  #Lets train the same batch of 1 20 times to see the loss value get reduced!!!\n",
    "    for batch in trainingBatch:\n",
    "        decoder_sequence = generate_decoder_input_sequence_per_encoder_batch(batch[1])\n",
    "        decoder_inputs = decoder_sequence['input_ids']\n",
    "        decoder_ouputs = decoder_sequence['output_ids']\n",
    "        total_batch_loss = 0\n",
    "        for i in range(len(decoder_sequence)):\n",
    "            Optimiser.zero_grad()\n",
    "            output = customTransformer(batch[0],decoder_inputs[i])\n",
    "            loss = Criterion(output,decoder_ouputs[i])\n",
    "            total_batch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            Optimiser.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch: ',epoch,'Loss: ',round(total_batch_loss,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d1e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yessssss, the the loss get reduced, which means the model can learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ed5214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do: Need to create a custom function to use the trained model, such as ... \n",
    "                         #...continously generating a sequence of outputs\n",
    "\n",
    "# Training can occur in parallel, but using the model can only occur sequentially as the model can\n",
    "# only generate one token as a time... Training is the hardest part!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dd618e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
